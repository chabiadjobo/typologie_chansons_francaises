{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numba\n",
    "numba.config.DISABLE_JIT = True # j'ai une erreur quand je ne vide pas le cahche de numba, probablement corrompu\n",
    "import dcor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "import numpy as np  # Nouvelle bibliothèque pour générer des modèles quadratiques et autres\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import itertools\n",
    "from statsmodels.stats.outliers_influence import reset_ramsey\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "from statsmodels.stats.stattools import durbin_watson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Fonction pour le test de normalité\n",
    "def test_normality(df, column):\n",
    "    stat, p = stats.shapiro(df[column].dropna())\n",
    "    is_normal = p > 0.05\n",
    "    print(f\"Test de normalité pour '{column}' : {'Normale' if is_normal else 'Non Normale'}\")\n",
    "    return is_normal\n",
    "\n",
    "# 2. Test de linéarité : Durbin-Watson\n",
    "def test_linearity(df, var1, var2):\n",
    "    X = sm.add_constant(df[var1])\n",
    "    model = sm.OLS(df[var2], X).fit()\n",
    "    dw_stat = sm.stats.durbin_watson(model.resid)\n",
    "    is_linear = (1.5 <= dw_stat <= 2.5)  # Indique l'absence d'autocorrélation (proche de 2)\n",
    "    print(f\"Test Durbin-Watson pour '{var1}' et '{var2}' : {'Linéaire' if is_linear else 'Non Linéaire'}\")\n",
    "    return is_linear\n",
    "\n",
    "# 3. Test de non-linéarité : RESET de Ramsey\n",
    "def test_non_linearity(df, var1, var2):\n",
    "    X = sm.add_constant(df[var1])\n",
    "    model = sm.OLS(df[var2], X).fit()\n",
    "    ramsey_test = reset_ramsey(model, degree=2)\n",
    "    is_nonlinear = ramsey_test.pvalue < 0.05\n",
    "    print(f\"Test RESET de Ramsey pour '{var1}' et '{var2}' : {'Non Linéaire' if is_nonlinear else 'Pas de non-linéarité détectée'}\")\n",
    "    return is_nonlinear\n",
    "\n",
    "# 4. Test pour vérifier la présence de \"ties\" dans Kendall\n",
    "def detect_ties(df, var1, var2):\n",
    "    kendall_corr, p_value = stats.kendalltau(df[var1], df[var2])\n",
    "    ties_count = len(df[var1].duplicated()) + len(df[var2].duplicated())\n",
    "    print(f\"Nombre de ties détectés : {ties_count}\")\n",
    "    return ties_count > 0\n",
    "\n",
    "# 5. Fonction pour évaluer la distribution\n",
    "def desequilibre_outliers(var_des):\n",
    "    \"\"\"\n",
    "    Détecte les déséquilibres dans les fréquences des modalités et les valeurs aberrantes.\n",
    "\n",
    "    Args:\n",
    "        data : Série de données à analyser.\n",
    "\n",
    "    Returns:\n",
    "        dict : Résultats de l'analyse des déséquilibres.\n",
    "    \"\"\"\n",
    "    # Vérification de la présence de données non nulles\n",
    "    if var_des.isnull().all():\n",
    "        return {\"is_balanced\": False, \"skewness\": np.nan, \"num_outliers\": 0}\n",
    "\n",
    "    # Calcul des fréquences des modalités\n",
    "    value_counts = var_des.value_counts()\n",
    "    total_counts = len(var_des)\n",
    "    \n",
    "    # Vérification de l'inégalité des fréquences\n",
    "    max_frequency = value_counts.max()\n",
    "    min_frequency = value_counts.min()\n",
    "\n",
    "    # Calcul de l'asymétrie\n",
    "    skewness = stats.skew(var_des.dropna())\n",
    "\n",
    "    # Détection des valeurs aberrantes\n",
    "    Q1 = np.percentile(var_des.dropna(), 25)  # Premier quartile\n",
    "    Q3 = np.percentile(var_des.dropna(), 75)  # Troisième quartile\n",
    "    IQR = Q3 - Q1  # Intervalle interquartile\n",
    "    \n",
    "    # Limites pour les valeurs aberrantes\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identification des valeurs aberrantes\n",
    "    outliers = var_des[(var_des < lower_bound) | (var_des > upper_bound)]\n",
    "\n",
    "    # Décision basée sur les résultats\n",
    "    decisions = {\n",
    "        \"is_balanced\": (max_frequency / min_frequency) < 10,\n",
    "        \"skewness\": skewness,\n",
    "        \"num_outliers\": len(outliers)  # Nombre de valeurs aberrantes\n",
    "    }\n",
    "    \n",
    "    return decisions\n",
    "\n",
    "\n",
    "# 5. Fonction pour choisir la méthode de corrélation automatiquement\n",
    "def choisir_correlation_auto(df, var1, var2, normal_var1, normal_var2):\n",
    "    \"\"\"\n",
    "    Choisit la méthode de corrélation appropriée entre deux variables.\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame contenant les données.\n",
    "        var1 : Nom de la première variable.\n",
    "        var2 : Nom de la deuxième variable.\n",
    "\n",
    "    Returns:\n",
    "        str : Méthode de corrélation sélectionnée (pearson, spearman ou kendall).\n",
    "    \"\"\"\n",
    "    # Vérification de la présence des colonnes\n",
    "    if var1 not in df.columns or var2 not in df.columns:\n",
    "        raise ValueError(f\"Les variables '{var1}' ou '{var2}' ne sont pas présentes dans le DataFrame.\")\n",
    "\n",
    "    # Cas où les deux variables sont normales\n",
    "    if normal_var1 and normal_var2:\n",
    "        if test_linearity(df, var1, var2):\n",
    "            return \"pearson\"\n",
    "        else:\n",
    "            return \"spearman\"\n",
    "\n",
    "    # Cas où l'une ou les deux variables ne sont pas normales\n",
    "    if test_non_linearity(df, var1, var2):\n",
    "        if desequilibre_outliers(df[var1])['is_balanced'] or desequilibre_outliers(df[var2])['is_balanced']:\n",
    "            return \"kendall\"\n",
    "\n",
    "    return \"spearman\"  # Méthode par défaut\n",
    "\n",
    "# 6. Fonction pour calculer la corrélation en fonction de la méthode choisie\n",
    "def calculer_correlation(method, df, var1, var2):\n",
    "    if method == \"pearson\":\n",
    "        return df[var1].corr(df[var2], method='pearson')\n",
    "    elif method == \"spearman\":\n",
    "        return df[var1].corr(df[var2], method='spearman')\n",
    "    elif method == \"kendall\":\n",
    "        return df[var1].corr(df[var2], method='kendall')\n",
    "    elif method == \"Quadratique\":\n",
    "        df['var1_squared'] = df[var1] ** 2\n",
    "        X = sm.add_constant(df[[var1, 'var1_squared']])\n",
    "        model = sm.OLS(df[var2], X).fit()\n",
    "        return model.rsquared  # Retourne R² pour l'ajustement quadratique\n",
    "    elif method == \"Distance Corrélation\":\n",
    "        return dcor.distance_correlation(df[var1].values, df[var2].values)\n",
    "    else:\n",
    "        raise ValueError(f\"Méthode de corrélation '{method}' non reconnue.\")\n",
    "\n",
    "# 7. Fonction pour la visualisation de la relation entre deux variables\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "def visualiser_relation(df, var1, var2, mode='manuel'):\n",
    "    \"\"\"\n",
    "    Cette fonction visualise et détecte automatiquement la relation entre deux variables (linéaire, quadratique, cubique).\n",
    "    En mode manuel, l'utilisateur peut choisir le type de relation.\n",
    "    En mode automatique, la fonction détecte le meilleur ajustement via des tests statistiques.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mode automatique : pas de visualisation, juste une détection automatique\n",
    "    if mode == 'auto':\n",
    "        X = sm.add_constant(df[var1])\n",
    "        y = df[var2]\n",
    "        \n",
    "        # Test de Durbin-Watson pour détecter une relation linéaire\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        dw_stat = durbin_watson(model.resid)\n",
    "        \n",
    "        # Test RESET pour détecter des non-linéarités\n",
    "        reset_test = linear_reset(model, power=2, use_f=False)\n",
    "        \n",
    "        print(f\"Durbin-Watson: {dw_stat}, Test RESET p-value: {reset_test.pvalue}\")\n",
    "        \n",
    "        # Si DW proche de 2 et RESET non significatif, on peut conclure à une relation linéaire\n",
    "        if 1.8 <= dw_stat <= 2.2 and reset_test.pvalue > 0.05:\n",
    "            return '1'  # Relation linéaire\n",
    "        elif reset_test.pvalue < 0.05:\n",
    "            # Test quadratique\n",
    "            poly_fit = np.poly1d(np.polyfit(df[var1], df[var2], 2))\n",
    "            r2_quad = np.corrcoef(df[var2], poly_fit(df[var1]))[0, 1] ** 2\n",
    "            poly_fit_cubic = np.poly1d(np.polyfit(df[var1], df[var2], 3))\n",
    "            r2_cubic = np.corrcoef(df[var2], poly_fit_cubic(df[var1]))[0, 1] ** 2\n",
    "            \n",
    "            # Comparaison R² quadratique et cubique\n",
    "            if r2_quad > r2_cubic:\n",
    "                return '2'  # Relation quadratique\n",
    "            else:\n",
    "                return '3'  # Relation cubique\n",
    "        else:\n",
    "            return '4'  # Non-linéaire ou autre\n",
    "\n",
    "    # Mode manuel : visualisation et choix manuel par l'utilisateur\n",
    "    else:\n",
    "        # Création de la figure avec plusieurs sous-graphiques\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        # 1. Ajustement linéaire\n",
    "        sns.regplot(x=df[var1], y=df[var2], ax=axes[0], line_kws={\"color\": \"red\"}, ci=None)\n",
    "        axes[0].set_title(\"Ajustement Linéaire\")\n",
    "        axes[0].set_xlabel(var1)\n",
    "        axes[0].set_ylabel(var2)\n",
    "\n",
    "        # 2. Ajustement quadratique\n",
    "        poly_fit = np.poly1d(np.polyfit(df[var1], df[var2], 2))\n",
    "        sns.scatterplot(x=df[var1], y=df[var2], ax=axes[1])\n",
    "        sns.lineplot(x=df[var1], y=poly_fit(df[var1]), ax=axes[1], color=\"green\")\n",
    "        axes[1].set_title(\"Ajustement Quadratique\")\n",
    "        axes[1].set_xlabel(var1)\n",
    "        axes[1].set_ylabel(var2)\n",
    "\n",
    "        # 3. Ajustement cubique\n",
    "        poly_fit_cubic = np.poly1d(np.polyfit(df[var1], df[var2], 3))\n",
    "        sns.scatterplot(x=df[var1], y=df[var2], ax=axes[2])\n",
    "        sns.lineplot(x=df[var1], y=poly_fit_cubic(df[var1]), ax=axes[2], color=\"blue\")\n",
    "        axes[2].set_title(\"Ajustement Cubique\")\n",
    "        axes[2].set_xlabel(var1)\n",
    "        axes[2].set_ylabel(var2)\n",
    "\n",
    "        plt.suptitle(f\"Comparaison des Ajustements pour {var1} et {var2}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Demande à l'utilisateur de choisir le type de relation\n",
    "        print(\"\\nQuel type de relation vous semble le plus adapté entre '{}' et '{}' ?\".format(var1, var2))\n",
    "        print(\"1 : Ajustement Linéaire\")\n",
    "        print(\"2 : Ajustement Quadratique\")\n",
    "        print(\"3 : Ajustement Cubique\")\n",
    "        print(\"4 : Non Linéaire ou Autre\")\n",
    "        relation_type = input(\"Choisissez une option (1, 2, 3 ou 4) : \")\n",
    "\n",
    "        return relation_type\n",
    "# 8. Fonction pour remplacer la probabilité par des étoiles\n",
    "def obtenir_significativite(p_value):\n",
    "    \"\"\"\n",
    "    Retourne les étoiles en fonction de la p-value.\n",
    "    \n",
    "    Args:\n",
    "    p_value : La p-value obtenue du test statistique\n",
    "    \n",
    "    Retourne :\n",
    "    Un string représentant le niveau de significativité ('***', '**', '*', ou '').\n",
    "    \"\"\"\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# 9. Fonction principale d'analyse de la corrélation (en mode automatique ou manuel)\n",
    "def analyser_correlation(df, var1, var2, mode='auto'):\n",
    "    \"\"\"\n",
    "    Analyse la corrélation entre deux variables.\n",
    "    En mode 'auto', la fonction détecte automatiquement la méthode de corrélation.\n",
    "    En mode 'manuel', l'utilisateur peut choisir la méthode après visualisation.\n",
    "    \n",
    "    Args:\n",
    "    df : DataFrame contenant les données\n",
    "    var1 : première variable (nom de la colonne)\n",
    "    var2 : deuxième variable (nom de la colonne)\n",
    "    mode : 'auto' pour un choix automatique de la méthode, 'manuel' pour choisir manuellement après visualisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test de normalité pour les deux variables\n",
    "    normal_var1 = test_normality(df, var1)\n",
    "    normal_var2 = test_normality(df, var2)\n",
    "\n",
    "    # Choisir la méthode de corrélation (auto ou manuel)\n",
    "    if mode == 'auto':\n",
    "        # Choix automatique de la méthode de corrélation\n",
    "        method = choisir_correlation_auto(df, var1, var2, normal_var1, normal_var2)\n",
    "    else:\n",
    "        # Visualisation et choix manuel\n",
    "        visualiser_relation(df, var1, var2)\n",
    "        print(\"Choisissez la méthode de corrélation :\")\n",
    "        print(\"1 : Pearson (linéaire, variables normales)\")\n",
    "        print(\"2 : Spearman (monotone, pas nécessairement normal)\")\n",
    "        print(\"3 : Kendall (si ties présents ou relation complexe)\")\n",
    "        print(\"4 : Distance Corrélation (relation non-linéaire)\")\n",
    "        choix = input(\"Entrez le numéro de la méthode : \")\n",
    "        \n",
    "        method = {\n",
    "            '1': 'pearson',\n",
    "            '2': 'spearman',\n",
    "            '3': 'kendall',\n",
    "            '4': 'Distance Corrélation'\n",
    "        }.get(choix, 'pearson')  # Par défaut, Pearson\n",
    "    \n",
    "    # Calcul de la corrélation avec la méthode choisie\n",
    "    correlation_value = calculer_correlation(method, df, var1, var2)\n",
    "    print(f\"Corrélation ({method}) entre {var1} et {var2} : {correlation_value}\")\n",
    "    \n",
    "    return correlation_value, method\n",
    "\n",
    "\n",
    "# 10. Fonction pour analyser plusieurs paires de variables\n",
    "\n",
    "\n",
    "def analyser_correlation_multiple(df, variables, mode='auto'):\n",
    "    \"\"\"\n",
    "    Analyse les corrélations entre toutes les combinaisons de variables dans une liste\n",
    "    et stocke les résultats dans un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df : DataFrame contenant les données\n",
    "    variables : Liste des noms des colonnes à analyser\n",
    "    mode : 'auto' pour choisir automatiquement la méthode, 'manuel' pour choisir manuellement\n",
    "    \n",
    "    Retourne : un DataFrame contenant les résultats de la corrélation avec les colonnes demandées.\n",
    "    \"\"\"\n",
    "\n",
    "    resultats = []\n",
    "\n",
    "    # Générer toutes les combinaisons possibles de variables\n",
    "    combinaisons = itertools.combinations(variables, 2)\n",
    "\n",
    "    for var1, var2 in combinaisons:\n",
    "        print(f\"\\nAnalyse de la corrélation entre '{var1}' et '{var2}' :\")\n",
    "\n",
    "        # Test de normalité pour les deux variables\n",
    "        normal_var1 = test_normality(df, var1)\n",
    "        normal_var2 = test_normality(df, var2)\n",
    "\n",
    "        # Choix automatique ou manuel de la méthode de corrélation\n",
    "        if mode == 'auto':\n",
    "            method = choisir_correlation_auto(df, var1, var2, normal_var1, normal_var2)\n",
    "        else:\n",
    "            visualiser_relation(df, var1, var2)\n",
    "            print(\"Choisissez la méthode de corrélation :\")\n",
    "            print(\"1 : Pearson (linéaire, variables normales)\")\n",
    "            print(\"2 : Spearman (monotone, pas nécessairement normal)\")\n",
    "            print(\"3 : Kendall (si ties présents ou relation complexe)\")\n",
    "            print(\"4 : Distance Corrélation (relation non-linéaire)\")\n",
    "            choix = input(\"Entrez le numéro de la méthode : \")\n",
    "            method = {\n",
    "                '1': 'pearson',\n",
    "                '2': 'spearman',\n",
    "                '3': 'kendall',\n",
    "                '4': 'Distance Corrélation'\n",
    "            }.get(choix, 'pearson')  # Par défaut Pearson\n",
    "\n",
    "        # Calcul de la corrélation en fonction de la méthode\n",
    "        if method == \"pearson\":\n",
    "            corr, p_value = stats.pearsonr(df[var1], df[var2])\n",
    "        elif method == \"spearman\":\n",
    "            corr, p_value = stats.spearmanr(df[var1], df[var2])\n",
    "        elif method == \"kendall\":\n",
    "            corr, p_value = stats.kendalltau(df[var1], df[var2])\n",
    "        elif method == \"Distance Corrélation\":\n",
    "            corr = dcor.distance_correlation(df[var1].values, df[var2].values)\n",
    "            p_value = np.nan  # La distance corrélation ne donne pas de p-value directement\n",
    "        else:\n",
    "            corr, p_value = np.nan, np.nan  # Si la méthode n'est pas reconnue\n",
    "\n",
    "        # Obtenir la significativité sous forme d'étoiles\n",
    "        significativite = obtenir_significativite(p_value)\n",
    "\n",
    "        # Stocker les résultats dans une liste\n",
    "        resultats.append({\n",
    "            'Variable 1': var1,\n",
    "            'Variable 2': var2,\n",
    "            'Valeur de Corrélation': corr,\n",
    "            'P-value': p_value,\n",
    "            'Méthode de Corrélation': method,\n",
    "            'Significativité': significativite\n",
    "        })\n",
    "    \n",
    "    # Créer un DataFrame à partir des résultats\n",
    "    df_resultats = pd.DataFrame(resultats, columns=['Variable 1', 'Variable 2', 'Valeur de Corrélation', 'P-value', 'Méthode de Corrélation', 'Significativité'])\n",
    "    \n",
    "    return df_resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "df = pd.read_csv('base_excel/data_scaled_zscore.csv')\n",
    "\n",
    "analyser_correlation(df, 'key_sin', 'key_cos', mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "df = pd.read_csv('base_excel/data_scaled_zscore.csv')\n",
    "liste_variable = ['danceability', 'energy', 'mode', 'acousticness', 'valence',\n",
    "       'tempo', 'release_year', 'key_sin', 'key_cos', 'loudness_winsorized',\n",
    "       'speechiness_log', 'instrumentalness_log', 'liveness_log',\n",
    "       'duration_log']\n",
    "result_correlation = analyser_correlation_multiple(df, liste_variable, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'valence',\n",
    "       'tempo', 'release_year', 'key_sin', 'key_cos', 'loudness_winsorized',\n",
    "       'speechiness_log', 'instrumentalness_log', 'liveness_log',\n",
    "       'duration_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
