{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies à importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ydata_profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 1 : Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Néttoyage de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('base_excel/songs_with_release_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['decenie'] = (data['release_year'] // 10) *10\n",
    "\n",
    "### Compter les fréquences par décennie\n",
    "frequency_by_decade = data['decenie'].value_counts().sort_index()\n",
    "\n",
    "### Afficher les résultats\n",
    "print(frequency_by_decade)\n",
    "#data = data[data['decenie'] >1950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## suppression des lignes \n",
    "data = data[data['release_year'] != 0] #où l'années est inférieur à 1960\n",
    "data = data[(data['duration_ms'] >= 73897.375) & (data['duration_ms'] <= 361032.375)]\n",
    "data = data[(data['liveness'] <= 0.8)] # Une valeur supérieure à 0,8 indique une forte probabilité que le morceau soit joué en direct.\n",
    "data = data[(data['instrumentalness'] <= 0.5)] # Une valeur supérieure à 0,5 indique une forte probabilité que le morceau soit un instrumentale\n",
    "data = data[(data['speechiness'] <= 0.66)] # Une valeur supérieure à 0,66 indique une forte probabilité que le morceau soit  constituées entièrement de mots parlés (talk-show, livre audio, poésie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suppression des variables\n",
    "variables_a_supprimer = ['pourcentage_langue3', 'langue3', 'pourcentage_langue2', 'langue2', 'langue1', 'pourcentage_langue1', 'album_name', 'artists', 'name', 'lyrics']\n",
    "data = data.drop(columns=variables_a_supprimer, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtenir_modalite=['key','mode']\n",
    "for var in obtenir_modalite:\n",
    "    modalites_uniques = data[var].unique()\n",
    "    print(f'les modalité de la variable {var} sont {modalites_uniques}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequence_key = data['key'].value_counts()\n",
    "frequence_mode = data['mode'].value_counts()\n",
    "print(frequence_key)\n",
    "print(frequence_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaires de mapping\n",
    "note_to_int = {\n",
    "    'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3,\n",
    "    'E': 4, 'F': 5, 'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8, 'Ab': 8,\n",
    "    'A': 9, 'A#': 10, 'Bb': 10, 'B': 11\n",
    "}\n",
    "\n",
    "mode_to_int = {\n",
    "    'Minor': 0, 'Major': 1\n",
    "}\n",
    "\n",
    "def convert_key(value, mapping):\n",
    "    try:\n",
    "        # Convertir les valeurs décimales en entiers\n",
    "        if isinstance(value, str) and '.' in value:\n",
    "            value = int(float(value))\n",
    "        # Convertir les notes musicales en entiers\n",
    "        return int(mapping.get(value, value))  # Retourner la valeur originale si elle n'est pas trouvée dans le dictionnaire\n",
    "    except ValueError:\n",
    "        # Si la conversion échoue, retourner la valeur originale\n",
    "        return value\n",
    "\n",
    "# Appliquer la conversion\n",
    "data['key'] = data['key'].apply(lambda x: convert_key(x, note_to_int))\n",
    "data['mode'] = data['mode'].apply(lambda x: convert_key(x, mode_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtenir_modalite=['key','mode']\n",
    "for var in obtenir_modalite:\n",
    "    modalites_uniques = data[var].unique()\n",
    "    print(f'les modalité de la variable {var} sont {modalites_uniques}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequence_key = data['key'].value_counts()\n",
    "frequence_mode = data['mode'].value_counts()\n",
    "print(frequence_key)\n",
    "print(frequence_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapport pandas profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(data, title=\"Pandas Profiling Report\")\n",
    "profile.to_file(\"rapport.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dd=data['release_year'].value_counts().sort_values()\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour identifier les outliers selon la méthode IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    is_outlier = (df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR))\n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour compter le nombre d'outliers par individu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers(df, variables_to_check):\n",
    "    # Initialiser une série qui va compter les outliers pour chaque individu\n",
    "    outliers_count = pd.Series(0, index=df.index)\n",
    "    \n",
    "    # Parcourir les colonnes et appliquer la fonction des outliers\n",
    "    for var in variables_to_check:\n",
    "        outliers_count += detect_outliers_iqr(df, var).astype(int)\n",
    "    \n",
    "    # Retourner le nombre d'outliers par individu\n",
    "    return outliers_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour générer les statistiques descriptives (avec nombre d'outliers par variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_numerical_vars(df, numeric_cols):\n",
    "    ##### Initialiser un DataFrame pour stocker les statistiques\n",
    "    stats = pd.DataFrame(index=numeric_cols, columns=[\n",
    "        'Moyenne', 'Médiane', 'Variance', 'Type de Distribution', 'Minimum', 'Maximum', 'Asymétrie', 'Nombre d\\'Outliers'\n",
    "    ])\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Calcul des statistiques de base\n",
    "        stats.loc[col, 'Moyenne'] = df[col].mean()\n",
    "        stats.loc[col, 'Médiane'] = df[col].median()\n",
    "        stats.loc[col, 'Variance'] = df[col].var()\n",
    "        stats.loc[col, 'Type de Distribution'] = 'Normale' if np.abs(df[col].skew()) < 0.5 else 'Non Normale'\n",
    "        stats.loc[col, 'Minimum'] = df[col].min()\n",
    "        stats.loc[col, 'Maximum'] = df[col].max()\n",
    "        stats.loc[col, 'Asymétrie'] = df[col].skew()\n",
    "        \n",
    "        # Détection des outliers via la fonction IQR\n",
    "        is_outlier = detect_outliers_iqr(df, col)\n",
    "        \n",
    "        # Stocker le nombre d'outliers pour chaque variable\n",
    "        stats.loc[col, 'Nombre d\\'Outliers'] = is_outlier.sum()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour afficher le tableau de fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_table(df, col):\n",
    "    col_values = df[col]\n",
    "    # Créer des intervalles\n",
    "    bins = np.linspace(col_values.min(), col_values.max(), 51)  # 50 intervalles\n",
    "    freq, edges = np.histogram(col_values, bins=bins)\n",
    "    \n",
    "    # Créer un DataFrame pour la fréquence\n",
    "    freq_table = pd.DataFrame({\n",
    "        'Intervalle': [f'({edges[i]:.2f}, {edges[i+1]:.2f}]' for i in range(len(edges)-1)],\n",
    "        'Fréquence': freq\n",
    "    })\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_use1 = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "#1. Calculer les statistiques\n",
    "stats_table = analyze_numerical_vars(data, variables_to_use1)\n",
    "print(stats_table)\n",
    "\n",
    "# 2. Calculer le nombre d'outliers par individu\n",
    "data['outliers_count'] = count_outliers(data, variables_to_use1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['outliers_count'] < 2)] # Une valeur supérieure à 0,8 indique une forte probabilité que le morceau soit joué en direct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_use1 = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculer les statistiques\n",
    "stats_table2 = analyze_numerical_vars(data, variables_to_use1)\n",
    "print(stats_table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for col in data.select_dtypes(include=[np.number]).columns:\n",
    "    print(f\"\\nTableau de fréquence pour {col}:\")\n",
    "    print(frequency_table(data, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transormation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encodage cyclique pour la varibale keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclic(value, N):\n",
    "    # N est le nombre de valeurs uniques (12 pour les notes musicales)\n",
    "    radians = 2 * np.pi * value / N\n",
    "    return np.sin(radians), np.cos(radians)\n",
    "\n",
    "# Appliquer la fonction à la colonne 'key'\n",
    "data['key_sin'], data['key_cos'] = zip(*data['key'].apply(lambda x: encode_cyclic(x, 12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorisation de loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_variable(data, column_name, mode='both'):\n",
    "    # Vérifier si la colonne existe\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"La colonne '{column_name}' n'existe pas dans le DataFrame.\")\n",
    "    \n",
    "    # Calculer les quartiles\n",
    "    Q1 = data[column_name].quantile(0.25)\n",
    "    Q3 = data[column_name].quantile(0.75)\n",
    "    \n",
    "    # Calculer l'IQR (écart interquartile)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculer les limites pour détecter les valeurs extrêmes\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Appliquer la Winsorisation manuelle en fonction du mode\n",
    "    winsorized_column_name = f\"{column_name}_winsorized\"\n",
    "    \n",
    "    if mode == 'both':\n",
    "        data[winsorized_column_name] = np.clip(data[column_name], lower_bound, upper_bound)\n",
    "    elif mode == 'upper':\n",
    "        data[winsorized_column_name] = np.clip(data[column_name], -np.inf, upper_bound)\n",
    "    elif mode == 'lower':\n",
    "        data[winsorized_column_name] = np.clip(data[column_name], lower_bound, np.inf)\n",
    "    else:\n",
    "        raise ValueError(\"Le mode doit être 'both', 'lower' ou 'upper'.\")\n",
    "    \n",
    "    # Afficher les statistiques avant et après la Winsorisation\n",
    "    variable_afficher = [column_name, winsorized_column_name]\n",
    "    print(f\"Avant et après Winsorisation pour '{column_name}':\\n\", analyze_numerical_vars(data, variable_afficher))\n",
    "    \n",
    "    print(f\"Limites : Supérieure = {upper_bound}, Inférieure = {lower_bound}\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = winsorize_variable(data, 'loudness', mode='both')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = winsorize_variable(data, 'tempo', mode='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = winsorize_variable(data, 'duration_ms', mode='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation logarithmique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speechiness_log'] = np.log(data['speechiness'] + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['speechiness_transformed'] = np.log(data['speechiness'] + 1)\n",
    "j'ai essayer ceci mais cela n'a pas réduit le nombre d'outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['instrumentalness_log'] = np.log(data['instrumentalness'] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['liveness_log'] = np.log(data['liveness'] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['duration_log'] = np.log(data['duration_ms'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse descriptives des variables transformer\n",
    "varr=['speechiness_log','instrumentalness_log','liveness_log', 'duration_log']\n",
    "print(analyze_numerical_vars(data, varr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score Normalization (StandardScaler) avec scikit-learn :\n",
    " loudness_winsorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour normaliser les colonnes numériques et afficher les statistiques\n",
    "def normalize_zscore(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Cette fonction applique la normalisation Z-score aux colonnes numériques spécifiées\n",
    "    et affiche les statistiques avant et après la normalisation.\n",
    "    \n",
    "    Paramètres:\n",
    "    - df: DataFrame contenant les données\n",
    "    - numeric_cols: Liste des colonnes numériques à normaliser\n",
    "    \n",
    "    Retour:\n",
    "    - df_scaled: DataFrame avec les colonnes normalisées\n",
    "    - stats_before: DataFrame des statistiques avant normalisation\n",
    "    - stats_after: DataFrame des statistiques après normalisation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialiser le scaler Z-score\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Afficher les statistiques avant normalisation\n",
    "    print(\"Statistiques avant normalisation :\")\n",
    "    stats_before = analyze_numerical_vars(df, numeric_cols)\n",
    "    print(stats_before)\n",
    "    \n",
    "    # Appliquer la normalisation Z-score aux colonnes numériques\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # Afficher les statistiques après normalisation\n",
    "    print(\"\\nStatistiques après normalisation :\")\n",
    "    stats_after = analyze_numerical_vars(df_scaled, numeric_cols)\n",
    "    print(stats_after)\n",
    "    \n",
    "    return df_scaled, stats_before, stats_after\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes numériques que tu veux normaliser\n",
    "numeric_cols = ['loudness_winsorized', 'speechiness_log', 'instrumentalness_log', 'liveness_log']\n",
    "\n",
    "# Appliquer la normalisation Z-score et afficher les statistiques avant et après\n",
    "data_scaled_zscore, stats_before, stats_after = normalize_zscore(data, numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### suppression des variables originales après transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_zscore = data_scaled_zscore.drop(data_scaled_zscore['loudness'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_zscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_des(colo):\n",
    "    # Initialiser un DataFrame pour stocker les statistiques\n",
    "    stats = pd.DataFrame(columns=[\n",
    "        'Moyenne', 'Médiane', 'Variance', 'Type de Distribution', 'Minimum', 'Maximum', 'Asymétrie', 'Outliers'\n",
    "    ])\n",
    "    \n",
    "    # Calcul des statistiques basiques\n",
    "    stats.loc['Valeurs', 'Moyenne'] = colo.mean()\n",
    "    stats.loc['Valeurs', 'Médiane'] = colo.median()\n",
    "    stats.loc['Valeurs', 'Variance'] = colo.var()\n",
    "    stats.loc['Valeurs', 'Type de Distribution'] = 'Normale' if np.abs(colo.skew()) < 0.5 else 'Non Normale'\n",
    "    stats.loc['Valeurs', 'Minimum'] = colo.min()\n",
    "    stats.loc['Valeurs', 'Maximum'] = colo.max()\n",
    "    stats.loc['Valeurs', 'Asymétrie'] = colo.skew()\n",
    "    \n",
    "    # Détection des valeurs extrêmes avec l'IQR\n",
    "    Q1 = colo.quantile(0.25)\n",
    "    Q3 = colo.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Nombre de valeurs extrêmes (outliers)\n",
    "    outliers = ((colo < (Q1 - 1.5 * IQR)) | (colo > (Q3 + 1.5 * IQR))).sum()\n",
    "    \n",
    "    # Stocker le nombre de valeurs extrêmes dans le tableau des statistiques\n",
    "    stats.loc['Valeurs', 'Outliers'] = outliers\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Appliquer la fonction sur la colonne 'loudness'\n",
    "loudness_apres = stat_des(data['loudness_winsorized'])\n",
    "print(loudness_apres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
